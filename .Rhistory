R = z + k*N
# Calculate spatial distribution given rent
pi_1 = 1*(w*A/R)^theta;
pi_1 = pi_1/sum(pi_1);
#  % Check convergence
diff = max(abs(pi_0-pi_1));
#  % Update rent
pi_0 = 0.7*pi_0 + 0.3*pi_1
#% counter
iter = iter + 1
}
N
R
db<-readRDS("C:\\Users\\Camila Cely\\Documents\\MECA\\INTERSEMESTRALES\\Big Data\\leverage_dta.Rds")
head(db)
require("tidyverse")
ggplot(db) +
geom_point(aes(x=x,y=y))
reg1<-lm(y~x,data=db)
summary(reg1)
require("stargazer")
stargazer(reg1,type="text")
db<- db %>% mutate(ej=c(rep(0,30),1))
head(db)
head(db)
tail(db)
reg2<-lm(y~x+ej,db)
stargazer(reg1,reg2,type="text")
db<-db %>% mutate(res_y_e=lm(y~ej,db)$residuals,
res_x_e=lm(x~ej,db)$residuals,
)
reg3<-lm(res_y_e~res_x_e,db)
stargazer(reg1,reg2,reg3,type="text")
db<-db %>% mutate(res_y_x=lm(y~x,db)$residuals, #Aquí sacamos los residuales de y contra x
res_e_x=lm(ej~x,db)$residuals, #y luego los de ej contra x
)
reg4<-lm(res_y_x~res_e_x,db) #y corremos esas dos cosas
stargazer(reg1,reg2,reg3,reg4,type="text")
u<-lm(y~x,data=db)$residual[31]
u
h<-lm.influence(reg1)$hat[31]
h
alpha<-u/(1-h)
alpha
install.packages("pacman")
## llamar librerias de la sesion
require(pacman)
p_load(rio, # import/export data
tidyverse, # tidy-data
skimr, # summary data
caret) # Classification And REgression Training
## set seed
set.seed(0000)
## load data
db <- import("https://gitlab.com/Lectures-R/bd-meca-2022-summer/lecture-01/-/raw/main/data/GEIH_sample1.Rds")
## data dictionary is available here:
browseURL("https://ignaciomsarmiento.github.io/GEIH2018_sample/dictionary.html")
vignette("tibble")
db <- as_tibble(db)
##=== 1. inspect data ===##
## view
head(db)
tail(db)
## summary db
skim(db)
require(skimr)
## summary var
summary(db$y_salary_m)
## data + mapping ##NOTA: GGPLOT VIENE CONTENIDA DENTRO DE TIDYVERSE
ggplot(data = db , mapping = aes(x = age , y = y_ingLab_m))
## + geometry
ggplot(data = db , mapping = aes(x = age , y = y_ingLab_m)) +
geom_point(col = "red" , size = 0.5) #AQUÍ SÍ LE DIGO QUÉ QUIERO QUE DIBUJE, PARA ESTE CASO ES UN SCATTER
ggplot(data = db ,
mapping = aes(x = age , y = y_ingLab_m , group=as.factor(formal) , color=as.factor(formal))) +
geom_point()
ggplot(data = db ,
mapping = aes(x = age , y = y_ingLab_m , group=as.factor(sex) , color=as.factor(sex))) +
geom_point()
p <- ggplot(data=db) +
geom_histogram(mapping = aes(x=y_ingLab_m , group=as.factor(sex) , fill=as.factor(sex)))
p
p + scale_fill_manual(values = c("0"="red" , "1"="blue") , label = c("0"="Hombre" , "1"="Mujer") , name = "Sexo")
box_plot <- ggplot(data=db , mapping = aes(as.factor(estrato1) , totalHoursWorked)) +
geom_boxplot()
box_plot
box_plot <- box_plot +
geom_point(aes(colour=as.factor(sex))) +
scale_color_manual(values = c("0"="red" , "1"="blue") , label = c("0"="Hombre" , "1"="Mujer") , name = "Sexo")
box_plot
## add theme
box_plot + theme_test()
h_hour = ggplot() + geom_histogram(data=db , aes(x=hoursWorkUsual) , fill="#99FF33" , alpha=0.5)
h_hour
View(h_hour)
h_hour + geom_histogram(data=db , aes(x=esc_hoursWorkUsual) , fill="#FF0066" , alpha=0.5)
db = db %>% mutate(esc_hoursWorkUsual = scale(hoursWorkUsual)) ##NOTAR QUE AQUI ESTAMOS MODIFICANDO DB
h_hour + geom_histogram(data=db , aes(x=esc_hoursWorkUsual) , fill="#FF0066" , alpha=0.5)
BoxCoxTrans(db$y_ingLab_m , na.rm=T)
ggplot() + geom_boxplot(data=db ,aes(x=y_ingLab_m) , fill="darkblue" , alpha=0.5) #AQUI VEMOS LA DISTRIBUCION QUE ACABAMOS DE HACER
ggplot() + geom_histogram(data=db ,aes(x=y_ingLab_m) , fill="darkblue" , alpha=0.5)
db = db %>% mutate(log_ingLab_m=log(y_ingLab_m)) #aqui lo transformamos logaritmicamente
ggplot() + geom_boxplot(data=db , aes(x=log_ingLab_m) , fill="coral1" , alpha=0.5)
quantile(x=db$p6426 , na.rm=T)
IQR(x=db$p6426 , na.rm=T)
iqr = IQR(x=db$p6426 , na.rm=T)
db_out = db %>% subset(p6426 <= 4*iqr)
cat("¡Elimina las NA!")
quantile(x=db_out$p6426 , na.rm=T)
nrow(db) - nrow(db_out)
db = db %>%
mutate(p6426_out = ifelse(test = p6426 > 4*iqr ,
yes = 1,
no = 0))
table(db$p6426_out)
q = quantile(db$p6426 , na.rm=T)
q
db = db %>%
mutate(p6426_q = case_when(p6426 < q[2] ~ "Q-1",
p6426 >= q[2] & p6426 < q[3] ~ "Q-2",
p6426 >= q[3] & p6426 < q[4] ~ "Q-3",
p6426 >= q[4] ~ "Q-4"))
table(db$p6426_q)
db %>% select(starts_with("p6"))
is.na(db$y_total_m)
is.na(db$y_total_m) %>% table()
db = db %>%
group_by(directorio) %>%
mutate(mean_y_total_m = mean(y_total_m,na.rm=T))#ese =T quiere decir que no se tengan en cuenta los NA, y cuando agrupamos por "directorio" es que agrupamos por vivienda
db %>% select(directorio,y_total_m,mean_y_total_m)
db = db %>%
mutate(ifelse(is.na(y_total_m)==T,
mean_y_total_m,
y_total_m))
db %>% select(directorio,y_total_m,mean_y_total_m)
install.packages("maptools", dependencies = TRUE)
install.packages("spdep", dependencies = TRUE)
install.packages("leaflet", dependencies = TRUE)
install.packages("RColorBrewer", dependencies = TRUE)
library(tidyverse)
library(mosaic)
install.packages(mosaic)
set.seed(10101)
sample1<-rbinom(n=100,size=1,p=.51)
sum(sample1)
library(mosaic)
samples<-do(10000)*sum(rbinom(n=100,size=1,p=.51))
samples<-do(10000)*sum(rbinom(n=100,size=1,p=.51)) #Aquí le decimos que haga el rbinom 10mil veces y sume,y ahi vemos la proporción en la que sale cada num
samples<-samples%>%mutate(prop=sum/100)
library(mosaic)
samples<-do(10000)*sum(rbinom(n=100,size=1,p=.51)) #Aquí le decimos que haga el rbinom 10mil veces y sume,y ahi vemos la proporción en la que sale cada num
samples<-samples%>%mutate(prop=sum/100)
hist(samples)
library(ggplot2)
plot(hist(samples))
install.packages(boot)
install.packages(bootstrap)
require("tidyverse")
db1<-fabricate(
N=10000
ability=rnorm(N,mean=.5,sd=2)
schooling=round(runif(N,2,14)),
logwage=rnorm(N,mean=7+.15*schooling, sd=2)
)
db1<-fabricate(
N=10000
ability=rnorm (N,mean=.5,sd=2)
schooling=round (runif(N,2,14)),
logwage=rnorm (N,mean=7+.15*schooling, sd=2)
)
require(mcmspatial)
install.packages(mcmspatial)
install.packages(caret)
require(pacman)
install.packages(pacman)
require(pacman)
p_load(tidyverse, # contiene las librerías ggplot, dplyr...
rvest)# web-scraping
## Acceder al robots.txt de wikipedia
browseURL("https://en.wikipedia.org/robots.txt")
my_html <-
'<!DOCTYPE html>
<html>
<meta charset="utf-8">
<head>
<title> Título de la página: ejemplo de clase </title>
</head>
<body>
<h1> Title 1.</h1>
<h2> Subtitle <u>subrayado-1</u>. </h2>
<p> Este es un párrafo muy pequeño que se encuentra dentro de la etiqueta <b>p</b> de <i>html</i> </p>
</body>
</html>'
write.table(x=my_html , file='my_page.html' , quote=F , col.names=F , row.names=F)
browseURL("my_page.html") ## leer con el navegador de su equipo
my_html <-
'<!DOCTYPE html>
<html>
<meta charset="utf-8">
<head>
<title> Título de la página: ejemplo de clase </title>
</head>
<body>
<h1> Title 1 Camila Cely.</h1>
<h2> Subtitle <u>subrayado-1</u>. </h2>
<p> Este es un párrafo muy pequeño que se encuentra dentro de la etiqueta <b>p</b> de <i>html</i> </p>
</body>
</html>'
write.table(x=my_html , file='my_page.html' , quote=F , col.names=F , row.names=F)
browseURL("my_page.html") ## leer con el navegador de su equipo
vignette("rvest")
my_url = "https://es.wikipedia.org/wiki/Copa_Mundial_de_F%C3%BAtbol"
browseURL(my_url) ## Ir a la página
my_html = read_html(my_url) ## leer el html de la página
class(my_html) ## ver la clase del objeto
View(my_html)
## Obtener los elementos h2 de la página
my_html %>% html_elements("h2")
## Ver los textos
my_html %>% html_elements("h2") %>% html_text()
my_html %>% html_nodes(xpath = '//*[@id="mw-content-text"]/div/p[1]') #este es el de clase
my_html %>% html_nodes(xpath = '//*[@id="mw-content-text"]/div/p[1]')  %>%
html_text()
## extraer todas las tablas del html
my_table = my_html %>% html_table()
## numero de tablas extraidas
length(my_table)
my_table[[11]]
sub_html = my_html %>% html_nodes(xpath='//*[@id="mw-content-text"]/div[1]/table[10]/tbody')
class(sub_html)
elements = sub_html %>% html_nodes("a")
elements[1:5]
titles = elements %>% html_attr("title")
titles[1:5]
refs = elements %>% html_attr("href")
refs[1:5]
vignette("rvest")
rm(list=ls())
install.packages(pacman)
require(pacman)
install.packages(pacman)
p_load(tidyverse,rvest)
url1<-"https://ignaciosarmiento.github.io/GEIH2018_sample/pages/geih_page_1.html"
browseURL(url1)
url1<-"https://ignaciomsarmiento.github.io/GEIH2018_sample/page1.html"
browseURL(url1)
tabla1<-url1%>% html_table()
my_html1 = read_html(url1)
View(my_html1)
class(my_html)
class(my_html1)
view(my_html1)
view(my_html1)
view(my_html1)
my_table1<-my_html1 %>% html_table()
View(my_table1)
length(my_table1)
my_html1 %>% html_nodes
my_html1 %>% html_nodes(xpath = '/html/body/div/div/div[2]/div/table')
my_table1<-my_html1 %>% html_table()
rm(list=ls())
url1<-"https://ignaciomsarmiento.github.io/GEIH2018_sample/page1.html"
tabla1<- url1 %>% read_html() %>% html_table()
tabla1<- url1 %>% read_html(url1) %>% html_table()
tabla1<- url1 %>% read_html() %>% html_table()
require(rvest)
tabla1<- url1 %>% read_html() %>% html_table()
vignette("rvest")
my_html1 <- read_html(url1)
class(my_html1)
my_html1 %>%
html_element("body") %>%
html_text2() %>%
cat()
my_html1 %>%
html_element("table") %>%
html_text2() %>%
cat()
class(my_html1)
tabla1<- url1 %>% read_html() %>% html_table()
length(tabla1)
install.packages(bootstrap)
install.packages(bootstrap)
install.packages(boot)
install.packages(boot)
library(boot, lib.loc = "C:/Program Files/R/R-4.1.1/library")
library(boot)
install.packages(caret)
library(caret)
install.packages("caret")
library("class")
library("MASS")
data(fgl)
str(fgl)
nearest1<-knn(train=x[-test], cl=fgl$type[-test], k=1)
x<-fgl
nearest1<-knn(train=x[-test], cl=fgl$type[-test], k=1) #aqui le decimos que nos pruebe, segun el mas cercano, cual seria la probabilidad
set.seed(1010101)
test<-sample(1:214,10) #aqui hay 10 obs solo para efectos de la clase
nearest1<-knn(train=x[-test], cl=fgl$type[-test], k=1) #aqui le decimos que nos pruebe, segun el mas cercano, cual seria la probabilidad
test<-sample(1:214,10)
nearest1<-knn(train=x[-test], cl=fgl$type[-test], k=1) #aqui le decimos que nos pruebe, segun el mas cercano, cual seria la probabilidad
x<-fgl
set.seed(1010101)
test<-sample(1:214,10) #aqui hay 10 obs solo para efectos de la clase
nearest1<-knn(train=x[-test], cl=fgl$type[-test], k=1) #aqui le decimos que nos pruebe, segun el mas cercano,
#cual seria la probabilidad
nearest5<-knn(train=x[-test], cl=fgl$type[-test], k=5)
data.frame(fgl$type[test],nearest1,nearest5)
data(fgl)
str(fgl)
x<-fgl
set.seed(1010101)
test<-sample(1:214,10) #aqui hay 10 obs solo para efectos de la clase
nearest1<-knn(train=x[-test], cl=fgl$type[-test], k=1) #aqui le decimos que nos pruebe, segun el mas cercano,
#cual seria la probabilidad
nearest5<-knn(train=x[-test], cl=fgl$type[-test], k=5)
data.frame(fgl$type[test],nearest1,nearest5) #todo esto todavia no está corriendo porque no alcance a copiar la ppt anterior
#pero la idea es que cada nearest nos muestra, de entre los x mas c
library("dplyr")
library("gamlr")
credit<-readRDS("credit.class.rds")
dim(credit)
library("gamlr")
install.packages(gamlr)
install.packages(gamlr)
library("gamlr")
install.packages(gamlr)
install.packages("gamlr")
library("gamlr")
credit<-readRDS("credit.class.rds") #voy a anotar el codigo pero no lo he podido correr
dim(credit)
library("dplyr")
library("dplyr")
library("gamlr")
credit<-readRDS("credit.class.rds") #voy a anotar el codigo pero no lo he podido correr
dim(credit)
chi.poly<-read_sf ("C:/Users/Camila Cely/Documents/MECA/INTERSEMESTRALES/Big Data/foreclosures/foreclosures.shp")
require("sf")
require("spdep")
require("dplyr")
chi.poly<-read_sf ("C:/Users/Camila Cely/Documents/MECA/INTERSEMESTRALES/Big Data/foreclosures/foreclosures.shp")
require(pacman)
p_load(tidyverse,
sf,
spdep,
dplyr)
chi.poly<-read_sf ("C:/Users/Camila Cely/Documents/MECA/INTERSEMESTRALES/Big Data/foreclosures/foreclosures.shp")
plot(chi.poly[violent])
plot(chi.poly[violent])
list.queen<-poly2nb(chi.poly, queen=TRUE) #armar una lista que muestra por ejemplo para la region 1 quienes son sus vecinos #vecinos del tipo reina, si uno pone false le sale torre
W<-nb2listw(list.queen, style="W", zero.policy=TRUE)
W
coords<-st_centroid(st_geometry(chi.poly), of_largest_polygon=TRUE)
W_dist<-dnearneigh(coords,C,1000)
W_dist
st.crs(chi.poly)<-4326
chi.poly<-st_transform(chi.poly,26916)
st_crs(chi.poly)<-4326
chi.poly<-st_transform(chi.poly,26916
coords<-st_centroid(st_geometry(chi.poly), of_largest_polygon=TRUE)
W_dist<-dnearneigh(coords,C,1000)   #aqui esto no me corre PORQUE NO ANOTE LA LINEA EN LA QUE SE PONIA EN GEOMETRIA PLANAR, BUSCAR EN PPT
W_dist
chi.poly<-st_transform(chi.poly,26916) #este numero es para chicago, buscar para bogota y medellin
list.queen<-poly2nb(chi.poly, queen=TRUE) #armar una lista que muestra por ejemplo para la region 1 quienes son sus vecinos #vecinos del tipo reina, si uno pone false le sale torre
W<-nb2listw(list.queen, style="W", zero.policy=TRUE) #si uno pusiera nb2mat le arma la matriz pero eso es computacionalmente muy costoso y hace lo mismo
W
#Characteristics of weights list object:
#  Neighbour list object:
#  Number of regions: 897
#Number of nonzero links: 6140
#Percentage nonzero weights: 0.7631036
#Average number of links: 6.845039 #EN PROMEDIO CADA QUIEN TIENE CASI 7 VECINOS
#Weights style: W
#Weights constants summary:
#  n     nn  S0       S1       S2
#W 897 804609 897 274.4893 3640.864
coords<-st_centroid(st_geometry(chi.poly), of_largest_polygon=TRUE)
W_dist<-dnearneigh(coords,C,1000)   #aqui esto no me corre PORQUE NO ANOTE LA LINEA EN LA QUE SE PONIA EN GEOMETRIA PLANAR, BUSCAR EN PPT
W_dist
W_dist<-dnearneigh(coords,C,1000)
chi.ols<-lm(violent~ est_fcs_rt + bls_unemp, data= chi.poly) #aqui empezamos a probar un modelo
summary(chi.ols)
moran.lm<-lm.morantest(chi.ols,W, alternative="two.sided") #alternativa de dos colas, la que ignacio suele hacer
print(moran.lm)
chi.poly<-st_transform(chi.poly,26916)
ist.queen<-poly2nb(chi.poly, queen=TRUE) #armar una lista que muestra por ejemplo para la region 1 quienes son sus vecinos #vecinos del tipo reina, si uno pone false le sale torre
W<-nb2listw(list.queen, style="W", zero.policy=TRUE) #si uno pusiera nb2mat le arma la matriz pero eso es computacionalmente muy costoso y hace lo mismo
W
coords<-st_centroid(st_geometry(chi.poly), of_largest_polygon=TRUE)
W_dist<-dnearneigh(coords,C,1000)   #aqui esto no me corre PORQUE NO ANOTE LA LINEA EN LA QUE SE PONIA EN GEOMETRIA PLANAR, BUSCAR EN PPT
sar.chi<-lagsarlm(violent~ est_fcs_rt + bls_unemp, data= chi.poly, W)
summary(sar.chi)
sar.chi<-lagsarlm(violent~ est_fcs_rt + bls_unemp, data= chi.poly, W)
sar.chi<- lagsarlm(violent~ est_fcs_rt + bls_unemp, data= chi.poly, W)
install.packages("spdep")
install.packages("spdep")
sar.chi<- lagsarlm(violent~ est_fcs_rt + bls_unemp, data= chi.poly, W)
summary(sar.chi)
sar.chi<- lagsarlm(violent~ est_fcs_rt + bls_unemp, data= chi.poly, W)
p_load(tidyverse,
sf,
spdep,
dplyr)
require(pacman)
p_load(tidyverse,
sf,
spdep,
dplyr)
sar.chi<- lagsarlm(violent~ est_fcs_rt + bls_unemp, data= chi.poly, W)
summary(sar.chi)
require(spdep)
sar.chi<- lagsarlm(violent~ est_fcs_rt + bls_unemp, data= chi.poly, W)
load("C:/Users/Camila Cely/Documents/MECA/INTERSEMESTRALES/Big Data/problem set 3/backup_jul22_workspace.RData")
## Llamar/instalar las librerías, usamos pacman para llamar e instalar si es necesario las librerias que necesitamos
require(pacman)
p_load(tidyverse,    #Para limpiar los datos
caret,        #Para la clasificación y regresiones
rio,          #Para importar datos
modelsummary, # msummary
gamlr,        # cv.gamlr >la que nos permite hacer lasso #NOTA ESTO ES LO QUE HAY QUE TENER CUIDADO, CARET Y GAMLR TIENEN FUNCIONES CON EL MISMO NOMBRE Y DISTINTO RESULTADO, REVISAR
class,
ggplot2,
skimr,
rvest,
dplyr,
stargazer,
gtsummary,
expss,
fastAdaboost,
randomForest,
xgboost,
glmnet,
pROC) #
load("C:/Users/Camila Cely/Documents/MECA/INTERSEMESTRALES/Big Data/problem set 3/backup_jul22_workspace.RData")
train_total_df <- train_total %>% st_drop_geometry()
train_total_df %>%
select(l3, price, property_type, area, dist_highway, dist_cbd) %>%
tbl_summary(by=l3)
colnames(train_total_df)
train_total_df <- train_total %>% st_drop_geometry()
require(pacman)
p_load(tidyverse,    #Para limpiar los datos
caret,        #Para la clasificación y regresiones
rio,          #Para importar datos
modelsummary, # msummary
gamlr,
class,
ggplot2,
skimr,
rvest,
dplyr,
stargazer,
gtsummary,
expss,
fastAdaboost,
randomForest,
xgboost,
glmnet,
pROC,
class,
sf,
leaflet,
tmaptools,
osmdata,
skim,
readr)
train_total_df <- train_total %>% st_drop_geometry()
colnames(train_total_df)
train_total_df %>%
select(l3, price, property_type, area, dist_highway_ciudades, dist_cbd_ciudades) %>%
tbl_summary(by=l3)
## clean environment
rm(list=ls())
###############################
###############################
## Llamar/instalar las librerias
require(pacman)
p_load(tidyverse,    #Para limpiar los datos
caret,        #Para la clasificación y regresiones
rio,          #Para importar datos
modelsummary, # msummary
gamlr,
class,
ggplot2,
skimr,
rvest,
dplyr,
stargazer,
leaflet,
haven,
hdm,
xtable,
sf,
gtsummary)
setwd("C:/Users/Camila Cely/Documents/GitHub/tesis_meca")
db <-readRDS("stores/BASE_TESIS.Rds")
summary (db)
mun <-read_sf("stores/Municipios/Municipios.shp")
mun <- st_transform(mun, 4326)
db_mun <- select(filter(db),c(codmpio, VIS10MIL, proporcionareaexpansion ))
db_mun <- db_mun %>% mutate (COD_MUNIC = codmpio )
mun <- mun %>% mutate (COD_MUNIC= as.numeric (mun$COD_MUNIC))
db_mun <- left_join(db_mun, mun)
summary(db_mun)
summary(mun)
class(db_mun)
class(mun)
db_mun <- st_as_sf (db_mun)
plot(db_mun)
leaflet() %>% addTiles() %>% addPolygons(
data= db_mun, color= "blue", fillOpacity=0.5, opacity = 0.5)
